# Data Pipeline Documentation

## Overview

The data pipeline automatically fetches crypto-related prediction markets from Polymarket every 15 minutes and stores them in the SQLite database. It includes error handling, logging, and monitoring capabilities.

## Architecture

### Components

1. **data_fetcher.py** - Main pipeline that runs on schedule
   - Fetches markets from Polymarket API
   - Filters for crypto-related markets
   - Stores/updates markets in database
   - Includes retry logic and comprehensive logging

2. **monitor.py** - Health check and status reporting
   - Checks last update time
   - Reports data freshness
   - Shows pipeline performance statistics
   - Exports JSON status for programmatic access

3. **logs/data_pipeline.log** - Pipeline activity log
   - All fetch attempts recorded
   - Errors and warnings logged
   - Useful for debugging and monitoring

4. **logs/pipeline_status.json** - Current status snapshot
   - Generated by monitor.py
   - JSON format for programmatic access
   - Includes freshness, stats, and performance metrics

## Quick Start

### 1. Install Dependencies

The schedule library is already in `requirements.txt`:

```bash
pip install -r requirements.txt
```

### 2. Start the Pipeline

Run the data fetcher in the background:

```bash
# On Windows (PowerShell)
Start-Process python -ArgumentList "src/data/data_fetcher.py" -NoNewWindow -RedirectStandardOutput logs/pipeline.log

# On Linux/Mac
python src/data/data_fetcher.py &
```

Or create a scheduled task (Windows) / cron job (Linux/Mac) to run it automatically.

### 3. Monitor Pipeline Health

Check the status anytime:

```bash
python src/data/monitor.py
```

This will display:
- Last update time and data freshness
- Market statistics
- Pipeline performance (total/successful/failed fetches)
- Recent errors and warnings
- Overall system status

## How It Works

### Fetch Cycle (Every 15 Minutes)

```
1. Connect to Polymarket API
   ‚Üì
2. Fetch active markets (limit: 100)
   ‚Üì
3. Filter for crypto keywords (BTC, ETH, SOL, etc.)
   ‚Üì
4. Extract market data (prices, volumes, status)
   ‚Üì
5. Store or update in database
   ‚Üì
6. Log results with timestamps
```

### Error Handling

The pipeline has built-in resilience:

- **Connection Errors**: Retries up to 3 times with exponential backoff
- **Timeout Errors**: Same retry mechanism
- **Database Errors**: Rolls back transactions on failure
- **Parsing Errors**: Gracefully skips malformed markets

### Logging

All activity is logged to `logs/data_pipeline.log`:

```
2026-01-16 20:50:58,737 - INFO - Starting market data fetch...
2026-01-16 20:50:58,792 - INFO - Fetching markets from Polymarket (attempt 1/3)
2026-01-16 20:51:15,150 - INFO - Successfully fetched 98 markets from Polymarket
2026-01-16 20:51:15,200 - INFO - Filtered 42 crypto markets from 98 total markets
2026-01-16 20:51:15,500 - INFO - Successfully stored/updated 42 markets
2026-01-16 20:51:15,501 - INFO - Data fetch completed successfully: 42 markets stored
```

## Monitoring

### Run Monitor Report

```bash
python src/data/monitor.py
```

Example output:

```
======================================================================
                    DATA PIPELINE MONITOR
======================================================================

üìä DATA FRESHNESS:
   Last Update: 2026-01-16 15:22:52 UTC
   Data Age: ‚úì FRESH
   Duration: 0:00:08.611336

üìà MARKET STATISTICS:
   Total Markets: 42
   Active Markets: 40
   Resolved Markets: 2
   Crypto Markets: 42

‚öôÔ∏è  PIPELINE PERFORMANCE:
   Total Fetches: 287
   Successful: 285
   Failed: 2
   Success Rate: 99.3%

‚úÖ SYSTEM STATUS:
   Pipeline: ‚úì RUNNING NORMALLY
```

### Data Freshness Categories

| Category | Age | Status |
|----------|-----|--------|
| Fresh | < 20 minutes | ‚úì OK - Pipeline working normally |
| Stale | 20 min - 1 hour | ‚ö† Warning - Some delays |
| Very Stale | > 1 hour | ‚úó Issue - Pipeline not updating |

## Testing

### Test with Mock Data

Test the database storage without hitting the API:

```bash
python test_pipeline_mock.py
```

Output:
```
Testing Data Pipeline with Mock Data
============================================================
Creating market: Will Bitcoin reach $100,000 by end of 2026?...
Creating market: Will Ethereum reach $5,000 by mid-2026?...
Creating market: Crypto Bull Run in 2026?...
‚úì Successfully stored/updated 3 markets
‚úì Total markets in database: 45
```

### View Recent Logs

```bash
# Windows PowerShell
Get-Content logs/data_pipeline.log -Tail 20

# Linux/Mac
tail -20 logs/data_pipeline.log
```

### Check Pipeline Status JSON

```bash
# View the status file
cat logs/pipeline_status.json
```

## Configuration

### Crypto Keywords (Filtered Markets)

The pipeline filters for these keywords (case-insensitive):
- Bitcoin: `bitcoin`, `btc`
- Ethereum: `ethereum`, `eth`
- Solana: `solana`, `sol`
- Dogecoin: `dogecoin`, `doge`
- XRP: `xrp`, `ripple`
- Cardano: `cardano`, `ada`
- Polygon: `polygon`, `matic`
- Arbitrum: `arbitrum`

To add more keywords, edit `src/data/data_fetcher.py`:

```python
CRYPTO_KEYWORDS = [
    'bitcoin', 'btc', 'ethereum', 'eth',
    # Add more here
]
```

### Schedule Interval

Default: Every 15 minutes

To change, edit `src/data/data_fetcher.py`:

```python
schedule.every(15).minutes.do(fetch_and_store_markets)
# Change to:
schedule.every(30).minutes.do(fetch_and_store_markets)  # 30 minutes
schedule.every(1).hours.do(fetch_and_store_markets)     # 1 hour
```

### API Timeout

Default: 15 seconds

To change, edit `src/data/data_fetcher.py`:

```python
TIMEOUT = 15  # Change this value
```

## Database Schema

Markets stored with these fields:

| Field | Type | Description |
|-------|------|-------------|
| market_id | String | Unique market identifier from Polymarket |
| question | Text | The market question (e.g., "Will Bitcoin hit $100k?") |
| yes_price | Float | Probability of YES outcome (0-1) |
| no_price | Float | Probability of NO outcome (0-1) |
| volume_24h | Float | 24-hour trading volume |
| volume | Float | Total trading volume |
| active | Boolean | Whether market is still active |
| resolved | Boolean | Whether market has been resolved |
| outcome | String | Resolved outcome (YES/NO) if applicable |
| created_at | DateTime | When market was first added |
| updated_at | DateTime | When market was last updated |

## Troubleshooting

### "Connection timeout" in logs

**Cause**: Polymarket API is unreachable or slow

**Solution**:
- Check internet connection
- Increase timeout in config (see Configuration section)
- Check if Polymarket API is down
- Pipeline will retry automatically 3 times

### "No crypto markets found"

**Cause**: API returned markets but none matched crypto keywords

**Solution**:
- Check Polymarket API is returning data
- Verify crypto keywords are correct
- Try increasing fetch limit (default: 100)

### "Database commit error"

**Cause**: Database is locked or corrupted

**Solution**:
- Close any other database connections
- Check disk space
- Verify permissions on data/ folder
- Delete locks: `rm data/.markets.db-shm` (if exists)

### Pipeline not running

**Cause**: Process terminated or not started

**Solution**:
- Check if process is still running: `ps aux | grep data_fetcher` (Linux)
- Check logs for errors: `tail -50 logs/data_pipeline.log`
- Restart the pipeline
- Check Python version: `python --version` (requires 3.7+)

## Production Deployment

### Running as a Service (Windows)

Use NSSM (Non-Sucking Service Manager):

```bash
nssm install PredictionMarketPipeline "C:\path\to\python.exe" "C:\path\to\src\data\data_fetcher.py"
nssm start PredictionMarketPipeline
```

### Running as a Service (Linux/Mac)

Create systemd service in `/etc/systemd/system/pipeline.service`:

```ini
[Unit]
Description=Prediction Market Data Pipeline
After=network.target

[Service]
Type=simple
User=your_user
WorkingDirectory=/path/to/project
ExecStart=/path/to/venv/bin/python src/data/data_fetcher.py
Restart=always
RestartSec=30

[Install]
WantedBy=multi-user.target
```

Then enable and start:

```bash
sudo systemctl enable pipeline
sudo systemctl start pipeline
sudo systemctl status pipeline
```

### Monitoring with Cron (Linux/Mac)

Add to crontab to check health every hour:

```bash
0 * * * * cd /path/to/project && python src/data/monitor.py >> logs/monitor.log 2>&1
```

## Performance

- **Typical fetch time**: 15-30 seconds
- **Markets per fetch**: 40-50 crypto markets
- **Database size**: ~1 MB per 1000 markets
- **Memory usage**: ~50-100 MB for the pipeline process

## Files

```
src/data/
‚îú‚îÄ‚îÄ data_fetcher.py      # Main pipeline (355 lines)
‚îú‚îÄ‚îÄ monitor.py           # Health monitoring (220 lines)
‚îú‚îÄ‚îÄ explore_polymarket.py # API exploration utility
‚îú‚îÄ‚îÄ test_apis.py         # API testing utilities
‚îî‚îÄ‚îÄ document_apis.py     # API documentation generator

logs/
‚îú‚îÄ‚îÄ data_pipeline.log    # Pipeline activity log
‚îî‚îÄ‚îÄ pipeline_status.json # Status snapshot (JSON)

data/
‚îî‚îÄ‚îÄ markets.db           # SQLite database with markets

tests/
‚îú‚îÄ‚îÄ test_pipeline_mock.py    # Mock data testing
‚îî‚îÄ‚îÄ test_data_pipeline.py    # Integration testing
```

## Next Steps

1. **Start the pipeline**: `python src/data/data_fetcher.py`
2. **Monitor status**: `python src/data/monitor.py`
3. **Build trading signals**: Use the market data in `src/ai/` module
4. **Execute trades**: Integrate with trading module

## Support

For issues or questions:
1. Check logs: `logs/data_pipeline.log`
2. Run monitor: `python src/data/monitor.py`
3. Test with mock data: `python test_pipeline_mock.py`
4. Review error handling in `src/data/data_fetcher.py`
